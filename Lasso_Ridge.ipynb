{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqIHbIG0yVJz/g8n+sn50E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzzzzuuuuu/big-data-analytics/blob/main/Lasso_Ridge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 라쏘 회귀, 릿지 회귀 (선형 모델을 이용한 지도학습)\n",
        "-> 선형 회귀에서 penalty만 줌\n",
        "\n",
        "## 정규화 (regularization)\n",
        "### sklearn.linear_model.SGDRegressor\n",
        "- penalty: {'l2', 'l1', 'elasticnet', None}, default='l2'\n",
        "- alpha 크면 클수록 정규화⬆️, 0이면 선형회귀\n",
        "\n",
        "### sklearn.linear_model.LogisticRegression\n",
        "- penalty: {'l1', 'l2', 'elasticnet', None}, default='l2'\n",
        "\n",
        "### 정규화\n",
        "- 모델의 학습(fit) 시에 과대 적합을 방지하여 성능 개선을 하는 방법\n",
        "- SGDRegressor 및 LogisticRegression 함수에서 penalty 옵션으로 정규화 방법을 정함\n",
        "  - 'l2'\n",
        "  - 'l1'\n",
        "  - 'elasticnet'\n",
        "  - None\n",
        "\n",
        "## 과대적합과 정규화\n",
        "### 과대적합 극복하기\n",
        "- 오차는 아래 두 값의 합으로 나타냄\n",
        "  - 편향(bias)\n",
        "    - 학습된 모델의 예측 값이 평균적으로 얼마나 틀렸는지\n",
        "    - 학습이 잘 되었는지 아닌지\n",
        "  - 분산(variance)\n",
        "    - 학습된 모델의 예측 값이 얼마나 다른지\n",
        "    - 일반화와 연관\n",
        "- 과대적합\n",
        "  - 모델이 학습 데이터에 과하게 적합화되어서 일반성이 떨어지는 상태\n",
        "  - 편향은 낮고, 분산이 큼\n",
        "- 과소적합\n",
        "  - 모델이 학습 데이터의 패턴을 찾지 못해 성능이 낮게 나오는 상태\n",
        "  - 편향은 크고, 분산이 낮음\n",
        "\n",
        "- 모델의 복잡도가 학습 데이터 수에 비해 클 때, 과대적합이 발생\n",
        "- 해결 방법\n",
        "  - 데이터 수 증가\n",
        "  - 데이터 전처리 혹은 피쳐수 감소 (모델 축소)\n",
        "  - 학습 조기 종료\n",
        "\n",
        "***정규화: 손실함수에 규제(penalty)를 두어 모델의 복잡도를 낮추는 방법**\n",
        "\n",
        "## 정규화\n",
        "### L1 정규화 *p.9\n",
        "- L1놈(norm)\n",
        "  - 각 계수의 절대값의 합\n",
        "- L1 정규화\n",
        "  - 가설함수(학습모델)가 복잡해지지 않도록 가설함수의 L1놈을 손실함수에 추가하여 커지지 않도록 하는 기법\n",
        "\n",
        "### L2 정규화 *10\n",
        "- L2놈(norm)\n",
        "  - 좌표 평면에서 원점으로부터의 거리\n",
        "- 가설함수(학습모델)가 복잡해지지 않도록 가설함수의 L2놈을 손실함수에 추가하여 커지지 않도록 하는 기법\n",
        "\n",
        "## 정규화와 라쏘 회귀, 릿지 회귀\n",
        "### L1 정규화\n",
        "- 두 선형 모델을 비교해보자.\n",
        "  - 모델A: 계수를 (2,-1,2,-1,2,-1)로 가짐 (L1놈 9)\n",
        "  - 모델B: 계수를 (1,1,1,1,1,1)로 가짐 (L1놈 6)\n",
        "\n",
        "  -> 모델B는 모델A에 비해 특정 피쳐에 더 많은 비중을 두기 때문에 과대적합이 될 확률이 낮음.\n",
        "\n",
        "  모델A는 특정 피쳐 값이 더 크고, 피쳐 간의 균형이 맞지 않아 과대적합 위험이 더 높음.\n",
        "\n",
        "  이런 경우 더 큰 L1-놈을 갖게 되므로, L1정규화는 손실함수에 L1놈을 추가하여 이런 경우 방지\n",
        "\n",
        "### L2 정규화\n",
        "- 두 선형 모델을 비교해보자.\n",
        "  - 모델A: 계수를 (1,1,1,1,1,1)로 가짐 (L2놈 sqrt(6))\n",
        "  - 모델B: 계수를 (2,0,2,0,2,0)로 가짐 (L2놈 sqrt(12)\n",
        "\n",
        "  -> 두 모델은 학습 데이터 (1,1,1,1,1,1), (2,2,2,2,2,2), (3,3,3,3,3,3)과 같은 데이터에 대해 동일한 예측값을 가짐\n",
        "\n",
        "  하지만 모델B는 모델A에 비해 특정 피쳐에 더 많은 비중을 두기 때문에 과대적합 될 확률이 높음.\n",
        "\n",
        "  이런 경우 더 큰 L2-놈을 갖게 되므로 L2정규화는 손실함수에 L2놈을 추가해서 이런 경우를 방지\n",
        "\n",
        "### 정규화 예시\n",
        "- L1 정규화\n",
        "  - 원점으로부터 가장 가까운 것 고르기\n",
        "- L2 정규화\n",
        "  - 영역 내 solution"
      ],
      "metadata": {
        "id": "iEAw1HSOQc-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라쏘 회귀\n",
        "- 선형회귀 + L1 정규화\n",
        "- 라쏘 회귀에서의 손실함수J *p.14\n",
        "  - 가설함수(선형)의 계수에 대해 L1놈을 추가하여, 복잡해지지 않게 함\n",
        "  - 패널티정도를 조절해주어야 함 (하이퍼파라미터)\n",
        "\n",
        "## 릿지 회귀\n",
        "- 선형회귀 + L2 정규화\n",
        "- 릿지 회귀에서의 손실함수J *p.15\n",
        "  - 가설함수(선형)의 계수에 대해 L2놈을 추가하여, 복잡해지지 않게 함\n",
        "  - 패널티정도를 조절해주어야 함 (하이퍼파라미터)\n",
        "\n",
        "## 싸이킷런 활용\n",
        "### 라쏘 회귀, 릿지 회귀를 포함한 선형 회귀 함수\n",
        "- LinearRegression\n",
        "  - 가장 기본적인 선형회귀 알고리즘을 사용하며, SGD가 아닌 최소자승법으로 계산\n",
        "- Lasso\n",
        "  - L1 손실을 활용한 라쏘 알고리즘 사용\n",
        "- Ridge\n",
        "  - L2 손실을 활용한 릿지 알고리즘 사용\n",
        "- **SGDRegressor**\n",
        "  - 확률적 경사 하강법을 사용한 회귀 모델을 만듦. SGD에서 비용함수만을 변경하여 모든 함수를 지원하고 있어 필요한 하이퍼 파라미터 매개변수를 설정\n",
        "    - None (선형)\n",
        "    - L1 (라쏘)\n",
        "    - L2 (릿지)\n",
        "\n",
        "### 라쏘 회귀, 릿지 회귀를 포함한 선형 회귀 함수 - #1\n",
        "- 다양한 정규화 파라미터에 따른 성능 비교"
      ],
      "metadata": {
        "id": "MFL31acnfupl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 라쏘 회귀, 릿지 회귀를 포함한 선형 회귀 함수"
      ],
      "metadata": {
        "id": "66ISZ0enCgm1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZnHOWtpDPgVr"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_diabetes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### diabete 데이터, 선형 회귀 분석"
      ],
      "metadata": {
        "id": "JFqiJnZ5DEVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = load_diabetes(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=1234)\n",
        "\n",
        "reg = SGDRegressor(penalty=None, max_iter=10000, learning_rate='constant', eta0=0.1, random_state=1234)\n",
        "reg = reg.fit(X_train, y_train)\n",
        "\n",
        "print(f'테스트 데이터셋 R2: {reg.score(X_test, y_test)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdBXTEU-CvsD",
        "outputId": "320060e6-6da3-4f10-d79b-568dd6d0c17d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "테스트 데이터셋 R2: 0.4644142372175787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### diabete 데이터, Lasso 회귀 분석"
      ],
      "metadata": {
        "id": "mbuLn63oD1H1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = load_diabetes(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=1234)\n",
        "\n",
        "# penalty=L1, alpha=정규화정도. alpha 값이 높을 수록 정규화가 높아지고, 낮을 수록 선형회귀에 가까워짐\n",
        "reg = SGDRegressor(penalty='l1', max_iter=10000, learning_rate='constant', eta0=0.1,\n",
        "                   alpha=0.001, random_state=1234)\n",
        "reg = reg.fit(X_train, y_train)\n",
        "\n",
        "print(f'테스트 데이터셋 R2: {reg.score(X_test, y_test)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_BkqzPbDyC4",
        "outputId": "f65fe2cf-5f4c-477c-b570-c402dfaa9d76"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "테스트 데이터셋 R2: 0.46445561210859265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### diabete 데이터, Ridge 회귀 분석"
      ],
      "metadata": {
        "id": "MCvYLQI1ETyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = load_diabetes(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=1234)\n",
        "\n",
        "reg = SGDRegressor(penalty='l2', max_iter=10000, learning_rate='constant', eta0=0.1,\n",
        "                   alpha=0.001, random_state=1234)\n",
        "reg = reg.fit(X_train, y_train)\n",
        "\n",
        "print(f'테스트 데이터셋 R2: {reg.score(X_test, y_test)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzjI2oAaESPM",
        "outputId": "b195ae9e-97c5-4cad-f80c-f0281c7f076d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "테스트 데이터셋 R2: 0.45183966904685113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression # 분류 모델"
      ],
      "metadata": {
        "id": "WhEIhTBmFFEy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tip 데이터\n"
      ],
      "metadata": {
        "id": "RI3r6tQmEk2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tips = pd.read_csv(\"./tips.csv\")\n",
        "tips.tip=pd.cut(tips['tip'],bins=3,labels=['low','midde','high'])\n",
        "tips['smoker']=tips['smoker'].map({'No':0,\"Yes\":1})\n",
        "tips['day']=tips['day'].map({'Thur':0,\"Fri\":1,\"Sat\":2,\"Sun\":3})\n",
        "tips['time']=tips['time'].map({'Lunch':0,\"Dinner\":1})\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(tips.drop(columns='tip'), tips['tip'],test_size=0.33,random_state=1234)"
      ],
      "metadata": {
        "id": "DaiBS5DwEipl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "clf = LogisticRegression(penalty=None, random_state=1234, solver='sag', max_iter=10000, multi_class='multinomial')\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_test = clf.predict(X_test_scaled)\n",
        "print(classification_report(y_test, y_pred_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTb1qh46FZwm",
        "outputId": "c1a95130-d81b-420f-c57c-f5417f145187"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        high       0.67      1.00      0.80         2\n",
            "         low       0.89      0.89      0.89        66\n",
            "       midde       0.42      0.38      0.40        13\n",
            "\n",
            "    accuracy                           0.81        81\n",
            "   macro avg       0.66      0.76      0.70        81\n",
            "weighted avg       0.81      0.81      0.81        81\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# L2정규화가 추가되었지만, C=1000으로 규제의 효과가 거의 없거나 무시되는 수준 => 규제가 없는 상태\n",
        "clf = LogisticRegression(penalty='l2', C=1000, random_state=1234, solver='sag', max_iter=10000, multi_class='multinomial')\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_test = clf.predict(X_test_scaled)\n",
        "print(classification_report(y_test, y_pred_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2FMab1BGHfA",
        "outputId": "72095f05-1a20-4792-e731-5a764cbea1d7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        high       0.67      1.00      0.80         2\n",
            "         low       0.89      0.89      0.89        66\n",
            "       midde       0.42      0.38      0.40        13\n",
            "\n",
            "    accuracy                           0.81        81\n",
            "   macro avg       0.66      0.76      0.70        81\n",
            "weighted avg       0.81      0.81      0.81        81\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C는 정규화 계수의 역수(C가 커질수록 패널티가 작아짐) -> 정규화가 작아짐."
      ],
      "metadata": {
        "id": "yqYaeJrVHHNs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6yqdBgxRGfnF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}